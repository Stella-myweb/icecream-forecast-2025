import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt

# 데이터 로드 (실제 파일 경로로 변경하세요)
df = pd.read_csv('ice_cream_sales_data.csv')

# 데이터 전처리
df['Month'] = pd.to_datetime(df['Month'])
df['Month_num'] = df['Month'].dt.month

# 특성과 타겟 분리
features = ['Temperature', 'Holidays', 'Promotion', 'Month_num']
X = df[features]
y = df['Sales']

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

# 스케일링 (SVM, KNN용)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 모델 초기화
models = {
    'SVM': SVR(kernel='rbf', C=100, gamma='scale'),
    'KNN': KNeighborsRegressor(n_neighbors=5),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)
}

# 결과 저장을 위한 딕셔너리
results = {}

# 각 모델 평가
for name, model in models.items():
    print(f"\n=== {name} 모델 ===")
    
    if name in ['SVM', 'KNN']:
        # 스케일된 데이터 사용
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')
    else:
        # Random Forest는 원본 데이터 사용
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    
    # 성능 지표 계산
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()
    
    results[name] = {
        'R²': r2,
        'RMSE': rmse,
        'MAE': mae,
        'CV_Mean': cv_mean,
        'CV_Std': cv_std
    }
    
    print(f"R² Score: {r2:.4f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"MAE: {mae:.2f}")
    print(f"CV Score: {cv_mean:.4f} (+/- {cv_std * 2:.4f})")

# 결과를 DataFrame으로 정리
results_df = pd.DataFrame(results).T
print("\n=== 모델 성능 비교 ===")
print(results_df.round(4))

# 성능 비교 시각화
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# R² Score 비교
axes[0].bar(results_df.index, results_df['R²'], color=['skyblue', 'lightgreen', 'lightcoral'])
axes[0].set_title('R² Score 비교')
axes[0].set_ylabel('R² Score')
axes[0].set_ylim(0, 1)

# RMSE 비교
axes[1].bar(results_df.index, results_df['RMSE'], color=['skyblue', 'lightgreen', 'lightcoral'])
axes[1].set_title('RMSE 비교 (낮을수록 좋음)')
axes[1].set_ylabel('RMSE')

# MAE 비교
axes[2].bar(results_df.index, results_df['MAE'], color=['skyblue', 'lightgreen', 'lightcoral'])
axes[2].set_title('MAE 비교 (낮을수록 좋음)')
axes[2].set_ylabel('MAE')

plt.tight_layout()
plt.show()

# 최고 성능 모델 확인
best_model = results_df['R²'].idxmax()
print(f"\n최고 성능 모델: {best_model} (R² = {results_df.loc[best_model, 'R²']:.4f})")

# Random Forest 특성 중요도 (최고 성능 모델인 경우)
if best_model == 'Random Forest':
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)
    
    feature_importance = rf_model.feature_importances_
    importance_df = pd.DataFrame({
        'Feature': features,
        'Importance': feature_importance
    }).sort_values('Importance', ascending=False)
    
    print("\n특성 중요도 (Random Forest):")
    print(importance_df)
    
    plt.figure(figsize=(8, 6))
    plt.bar(importance_df['Feature'], importance_df['Importance'])
    plt.title('Random Forest 특성 중요도')
    plt.xlabel('특성')
    plt.ylabel('중요도')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
